%-*-coding: utf-8;-*-
\documentclass[italian,a4paper]{scrartcl}
\usepackage{amsmath,amssymb,amsthm,thmtools}
\usepackage{eucal,babel,a4}
\usepackage[dvipsnames]{xcolor}
\usepackage[nochapters]{classicthesis}
\usepackage[utf8]{inputenc}
\usepackage{verbatim} % for the comment environment

\newcommand{\RR}{{\mathbb R}}
\newcommand{\ZZ}{{\mathbb Z}}
\newcommand{\C}{{\mathcal C}}
\newcommand{\eps}{\varepsilon}
\newcommand{\defeq}{=}
\renewcommand{\vec}{\mathbf}
\renewcommand{\div}{\mathrm{div}}
\newcommand{\rot}{\mathbf{rot}\,}
\newcommand{\vecnabla}{\mathbf{\nabla}}
\newcommand{\tr}{\mathrm{tr}}

\def\Xint#1{\mathchoice
{\XXint\displaystyle\textstyle{#1}}%
{\XXint\textstyle\scriptstyle{#1}}%
{\XXint\scriptstyle\scriptscriptstyle{#1}}%
{\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
\!\int}
\def\XXint#1#2#3{{\setbox0=\hbox{$#1{#2#3}{\int}$ }
\vcenter{\hbox{$#2#3$ }}\kern-.6\wd0}}
\def\dashint{\Xint-}

\declaretheoremstyle[
spaceabove=6pt, spacebelow=6pt,
headfont=\normalfont\itshape,
notefont=\mdseries, notebraces={(}{)},
bodyfont=\normalfont,
postheadspace=1em,
qed=,
shaded={rulecolor=pink!30,rulewidth=1pt,bgcolor=pink!10}
]{mystyle}

\declaretheorem[numberwithin=section,name=Teorema]{theorem}
\declaretheorem[sibling=theorem,name=Lemma]{lemma}
\declaretheorem[sibling=theorem,name=Definizione]{definition}
\declaretheorem[style=mystyle,sibling=theorem,name=Esercizio]{exercise}
\declaretheorem[style=mystyle,sibling=theorem,name=Esempio]{example}


\title{Forme differenziali}
\author{E. Paolini}
\date{26 ottobre 2014}

\begin{document}
\maketitle

\section{Spazio duale}

Se $V$ è uno spazio vettoriale reale di dimensione $n$, chiamiamo
\emph{spazio duale di $V$} che indichiamo con $V^* =
\mathcal L (V,\RR)$ lo spazio vettoriale delle applicazioni lineari
continue definite su $V$ a valori in $\RR$.

Se $\vec{e_1},\dots, \vec{e_n}$ è una base di $V$ ogni vettore $v$ si può scrivere
nella forma
\[
  \vec v = \sum_{j=1}^n v_j \vec {e_j}
\]
(i coefficienti $v_j$ vengono chiamate coordinate del vettore $\vec v$
nella base $\vec{e_1},\dots, \vec{e_n}$).

In corrispondenza di $\vec{e_1},\dots, \vec{e_n}$ si può dunque definire una base
dello spazio duale $\omega_1,\dots,\omega_n$ ponendo
\[
  \omega_i (\vec v) \defeq v_i
\]
se $\vec v = \sum_i v_i \vec{e_i}$.
Ovvero:
\[
  \omega_i(\vec{e_j}) = \delta_{ij}
\]
dove $\delta_{ij}$ vale $1$ se $i=j$ e $0$ se $i\neq j$ (delta di
Kronecker).

Se sullo spazio vettoriale $V$ mettiamo un prodotto scalare $(\cdot
,\cdot)$ ogni elemento $\vec v\in V$ ci fornisce una applicazione lineare
$\omega \in V^*$ definita da $\omega(\vec w) = (\vec v,\vec w)$. Visto che questa
corrispondenza tra vettori dello spazio $V$ e vettori dello spazio
$V^*$ è iniettiva, e visto che $V$ e $V^*$ hanno la stessa dimensione,
ogni elemento di $V^*$ può essere rappresentato in questo modo.

\section{Forme differenziali lineari}

Consideriamo ora un aperto $\Omega\subset \RR^n$ e definiamo
$\Lambda^1(\Omega)$ (in certi testi $\Omega^1(\Omega)$) lo spazio
vettoriale di tutte le funzioni $\omega\colon \Omega \to (\RR^n)^*$.
Queste funzioni si chiamano \emph{forme differenziali lineari} o
anche \emph{$1$-forme differenziali}.

Gli elementi di $\Lambda^1(\Omega)$ sono quindi delle funzioni
$\omega$ che ad
ogni punto del dominio restituiscono una applicazione lineare:
$\omega(x) \in (\RR^n)^*$, $\omega(x)(\vec v) \in \RR$. Per distinguere le
due variabili da cui dipende $\omega$ si usano a volte delle notazioni
alternative, come ad esempio le seguenti:
\[
\omega(x)(\vec v) = \omega_x(\vec v) = \omega(x)[\vec v].
\]
Rimarchiamo il fatto che $\omega_x(\vec v)$ dipende da $\vec v$ in modo
lineare, mentre in generale la dipendenza da $x$ potrebbe non essere lineare.

L'esempio più importante di \emph{forma differenziale} è il
differenziale di una funzione.
Consideriamo una funzione differenziabile $f\colon \Omega\subset
\RR^n\to \RR$. Dalla definizione di differenziabilità sappiamo che in
corrispondenza di ogni punto $x\in \Omega$ esiste una applicazione
lineare $L\colon \RR^n \to \RR$ che soddisfa la relazione:
\[
\lim_{x\to x_0} \frac{f(x) - f(x_0) - L(\vec
  x-x_0)}{\lvert x-x_0\rvert} = 0.
\]
La mappa $L$ può essere denotata in diversi modi:
\[
L (\vec v)
= Df(x_0) \vec v
= (\vecnabla f(x_0),\vec v)
= \sum_{k=1}^n \frac{\partial f}{\partial x_k}(x_0) \, v_k
= \frac{\partial f}{\partial v}(x_0)
= df_{x_0}(\vec v).
\]
Le diverse notazioni rappresentano modi leggermente diversi per
rappresentare lo stesso oggetto: $Df$ è la matrice $1\times n$
(vettore riga) che
rappresenta l'applicazione lineare $L$ nelle coordinate canoniche di
$\RR^n$; il vettore $\vecnabla f$ è quel vettore (vettore colonna)
che rappresenta
l'applicazione lineare $L$ tramite il prodotto scalare. Nell'ultima
notazione $df$ rappresenta invece l'applicazione lineare stessa ed è
quindi proprio una forma differenziale lineare, $df\in \Lambda^1(\Omega)$.

Consideriamo ora la funzione $x_k\colon \Omega\to \RR$ che al punto
$x\in \Omega$ associa la sua $k$-esima coordinata: $x_k$.
Osserviamo che facendo il differenziale $dx_k$ della funzione $x_k$ in
qualunque punto $x \in \RR^n$,
si ottiene esattamente l'elemento $\omega_k$ della base canonica di
$(\RR^n)^*$. Infatti:
\[
dx_k(\vec{e_j}) = \frac{\partial x_k}{\partial x_j} = \delta_{kj}.
\]
In particolare $dx_k(\vec{v}) = v_k$ se $v_k$ sono le coordinate di
$\vec{v}$ nella base canonica.

Dunque ogni forma differenziale $\omega \in \Lambda^1(\Omega)$ può
essere scritta nel seguente modo:
\begin{equation}\label{eq:componenti_forma}
   \omega(x) = \sum_{k=1}^n a_k(x) dx_k.
\end{equation}
In particolare il differenziale di una funzione si può scrivere così:
\[
  df_{x} = \sum_{k=1}^n \frac{\partial f}{\partial x_k} dx_k
\]
in quanto 
$df_{x}(\vec v) 
= \sum_{k=1}^n v_k \cdot \frac{\partial f}{\partial
  x_k}(x)$
e $v_k = dx_k(v)$.

Il prodotto scalare canonico di $\RR^n$ ci permette di mettere in
corrispondenza le forme differeniali lineari con i \emph{campi
  vettoriali}.
Un campo vettoriale è una funzione $\xi \colon \Omega\subset \RR^n \to
\RR^n$. Al campo $\xi$ possiamo associare una forma differenziale
$\omega$ definita come segue:
\[
  \omega_{x}(\vec v) = (\xi(x),\vec v).
\]
Se le componenti di $\omega$ sono le funzioni $a_k$ date dalla
\eqref{eq:componenti_forma} le stesse funzioni sono le coordinate di
$\xi$ nella base canonica di $\RR^n$:
\[
  \xi(x) = \sum_{k=1}^n a_k(x) \vec{e_k}.
\]


\section{Integrale di linea di una forma differenziale}

Data $\omega\in \Lambda^1(\Omega)$ e data una curva regolare a tratti
$\gamma\colon [a,b]\to \Omega$, definiamo l'integrale di $\omega$
lungo $\gamma$ tramite la formula:
\[
\int_\gamma \omega = \int_a^b \omega_{\gamma(t)}(\gamma'(t))\, dt.
\]

Osserviamo immediatamente che se $p\colon[c,d]\to [a,b]$ è una
riparametrizzazione regolare, crescente, posto $\eta(\tau) =
\gamma(p(\tau))$ si ha (tramite il cambio di variabili $t=p(\tau)$)
\begin{align*}
\int_\gamma \omega
&= \int_a^b \omega_{\gamma(t)}(\gamma'(t))\, d t
= \int_c^d \omega_{\gamma(p(\tau))}(\gamma'(p(\tau))\, p'(\tau)\, d\tau\\
&= \int_c^d \omega_{\eta(\tau)} ((\gamma\circ p)'(\tau))\, d\tau
= \int_\eta \omega.
\end{align*}
Dunque l'integrale di una forma differenziale su due curve equivalenti
e concordi, è lo stesso. Se invece le due curve hanno orientazione
opposta cioè se $p'(t)<0$, con il cambio di variabili gli estremi di
integrazione si scambiano, in quanto $p(c)=b$ e $p(d)=a$, dunque si
otterrà $\int_\eta \omega = - \int_\gamma \omega$.

Dunque, a differenza dell'integrale curvilineo, l'integrale di una
forma differenziale dipende dall'orientazione della curva.

Se la forma differenziale $\omega$ è associata al campo vettoriale
$\xi$ si ha
\[
  \int_\gamma \omega
= \int_a^b (\xi, \gamma'(t))\, dt
= \int_\gamma (\xi, \tau)\, ds
\]
dove $\tau(\gamma(t)) = \gamma'(t) / \lvert \gamma'(t)\rvert$ è
il versore tangente alla curva $\gamma$ con verso concorde
all'orientazione. In particolare quando la curva $\gamma$ è chiusa,
l'integrale di linea si chiama anche
\emph{circuitazione} del campo $\xi$ lungo la curva $\gamma$.

Osserviamo che se $\omega = df$ è un differenziale, si
ha\footnote{Ricordiamo la derivata della funzione composta:
\[
\frac{d}{dt} f(\gamma(t)) = Df(\gamma(t))\cdot \gamma'(t) = df_{\gamma(t)}(\gamma'(t)).
\]}
\[
\int_\gamma df = \int_a^b df_{\gamma(t)}(\gamma'(t))\, dt
= \int_a^b \frac{d}{dt} f(\gamma(t))\, dt = f(\gamma(b)) - f(\gamma(a))
\]
cioè l'integrale non dipende neanche dalla curva, ma solo dai suoi
estremi. Se poi la curva $\gamma$ è chiusa, si ha $\int_\gamma df = 0$.

\begin{example}
Consideriamo la forma differenziale $\omega = y^2 \, dx$ in $\RR^2$. Sia $\gamma$
la curva che descrive il perimetro del quadrato $[0,1]\times[0,1]$ in
senso antiorario. Calcoliamo $\int_\gamma \omega$. Visto che
l'integrale è additivo rispetto al dominio, è sempre possibile
spezzare una curva in più componenti. In particolare in questo caso il
perimetro del quadrato può essere spezzato nei quattro lati che lo
compongono. Questo evita anche il problema di trovare una
parametrizzazione di classe $\C^1$ che attraversi i vertici del
quadrato.

Dunque $\int_\gamma \omega = \int_{\gamma_1} \omega + \int_{\gamma_2}
\omega + \int_{\gamma_3} \omega + \int_{\gamma_4} \omega$ dove per
$t\in [0,1]$ poniamo $\gamma_1(t)=(t,0)$, $\gamma_2(t)=(1,t)$,
$\gamma_3(t)=(1-t,1)$, $\gamma_4(t)=(0,1-t)$.
Si ha allora
\begin{align*}
\int_{\gamma_1} \omega = \int_0^1 0^2\, dt = 0 &
\int_{\gamma_2} \omega = \int_0^1 t^2 \cdot 0\, dt = 0 \\
\int_{\gamma_3} \omega = \int_0^1 1^2 (-1)\, dt = -1 &
\int_{\gamma_4} \omega = \int_0^1 (1-t)^2\cdot 0\, dt = 0
\end{align*}
e quindi $\int_\gamma \omega = -1$.
\end{example}

\begin{example}
Consideriamo la forma differenziale $\omega = 2x\, dx + 2y \,
dy$ e la curva $\gamma(t) =
(x(t),y(t))$ con $x(t) = t$, $y(t)=t^2$ per $t\in [0,1]$. Si ha allora
\begin{align*}
\int_\gamma \omega
&= \int_\gamma 2x\, dx + 2y\, dy
= \int_0^1 2x(t) x'(t) + 2y(t) y'(t) \, dt\\
&= \int_0^1 2 t + 2 t^2\cdot 2t \, dt
= [ t^2 + t^4 ]_0^1 = 2.
\end{align*}
Osserviamo ora che posto $f(x,y)=x^2+y^2$ si ha $df = 2x\, dx + 2y\, dy =
\omega$. Dunque si può semplificare il calcolo sapendo che:
\[
 \int_\gamma \omega = \int_\gamma df = f(\gamma(1)) - f(\gamma(0))
 = f(1,1) - f(0,0) = 2.
\]
\end{example}

\section{forme chiuse e forme esatte}
Una forma differenziale $\omega$ si dice essere \emph{esatta} se
esiste una funzione $f$ (chiamata \emph{primitiva}) tale che $df = \omega$. Abbiamo visto nel
paragrafo precedente che l'integrale di una forma differenziale esatta
dipende solo dagli estremi della curva (e dall'orientazione) e non dal
percorso fatto. Nel linguaggio dei campi vettoriali quando si ha $\xi =
\vecnabla f$ si dice che il campo $\xi$ è \emph{conservativo} e $f$
è un \emph{potenziale} di $\xi$.

Visto che trovare un potenziale di una forma differenziale semplifica
il calcolo degli integrali di linea, è importante avere delle
condizioni per identificare le forme esatte.

Osserviamo che se $\omega = df$ è una forma differenziale di classe
$\mathcal C^1$:
\[
 \omega = \sum_{k=1}^n a_k dx_k
\]
si avrà $a_k = \frac{\partial f}{\partial x_k}$ 
con $f$ di classe $C^2$, dunque per il teorema
di Schwarz sulle derivate seconde miste, si avrà:
\[
 \frac{\partial a_k}{\partial x_j} = \frac{\partial^2 f}{\partial x_j
   \partial x_k} = \frac{\partial^2 f}{\partial x_k \partial x_j}
=\frac{\partial a_j}{\partial x_k}.
\]

Diremo che la forma differenziale $\omega = \sum a_k dx_k$ è
\emph{chiusa}, se soddisfa le equazioni:
\[
  \frac{\partial a_k}{\partial x_j} = \frac{\partial a_j}{\partial
    x_k}
\]
per ogni $j=1,\dots, n$ e ogni $k=1,\dots,n$.

Per l'osservazione che abbiamo fatto, sappiamo che vale il seguente.
\begin{theorem}
Se $\omega$ è una forma esatta, di classe $C^1$, 
allora $\omega$ è chiusa.
\end{theorem}

\begin{example}
\label{ex:forma_chiusa_non_esatta}
Consideriamo la forma differenziale
\begin{equation}\label{eq:forma_chiusa_non_esatta}
\omega = \frac{-y\, dx + x\,
  dy}{x^2+y^2}
\end{equation}
che risulta essere definita su $\RR^2 \setminus \{(0,0)\}$ e consideriamo
la circonferenza $\gamma(t)=(x(t),y(t))$ con $x(t) = R \cos t$, $y(t)=R
\sin t$ al variare di $t\in [0,2\pi]$.
Si ha
\begin{align*}
\int_\gamma \omega & = \int \frac{-y\, dx + x\, dy}{x^2+y^2}
=\int_0^{2\pi} \frac{-y(t) x'(t) + x(t) y'(t)}{x^2(t)+y^2(t)}\, dt\\
&=\int_0^{2\pi} \frac{R^2 \sin^2 t + R^2 \cos^2 t}{R^2\cos^2 t +
  R^2\sin^2 t}\, dt = \int_0^{2\pi}\, dt = 2\pi.
\end{align*}
Possiamo affermare quindi che $\omega$ non è esatta, in quanto
c'è almeno una curva chiusa su cui l'integrale non è zero.
Controlliamo se la forma è
chiusa:
\begin{align*}
\frac{\partial}{\partial y}\frac{-y}{x^2+y^2}
& = \frac{-(x^2+y^2)+y2y}{(x^2+y^2)^2}
= \frac{y^2 - x^2}{(x^2+y^2)^2}\\
\frac{\partial}{\partial x}\frac{x}{x^2+y^2}
& = \frac{x^2+y^2-x2x}{(x^2+y^2)^2}
= \frac{y^2 - x^2}{(x^2+y^2)^2}
\end{align*}
dunque risulta che la forma è chiusa.
\end{example}

Diremo che un aperto $\Omega\subset \RR^n$ è \emph{connesso per archi}
se dati due punti qualunque in $\Omega$ esiste una curva di classe
$\mathcal C^1$ i cui estremi sono i punti dati.

\begin{theorem}
Se $\omega$ è una forma differenziale, continua, definita su un aperto
connesso per archi $\Omega\subset \RR^n$ allora le seguenti proprietà
sono equivalenti:
\begin{enumerate}
\item $\omega$ è esatta;
\item $\int_\gamma \omega = 0$ per ogni curva chiusa $\gamma$ di
  classe $\mathcal C^1$;
\item $\int_\gamma \omega = \int_\eta \omega$ se $\gamma$ e $\eta$
  sono curve di classe $\mathcal C^1$ con lo stesso punto iniziale e lo
  stesso punto finale.
\end{enumerate}
\end{theorem}

\begin{proof}
Abbiamo già osservato che 1 implica 2 in quanto l'integrale di
una forma esatta è la differenza del valore della primitiva nei suoi estremi.

Dimostriamo che 2 implica 3. Siano $\eta$ e $\gamma$ curve
$\mathcal C^1$ con gli stessi estremi. Vogliamo costruire una curva
chiusa concatendando $\eta$ con la curva $\gamma$ percorsa in senso
inverso. A meno di riparametrizzazioni possiamo supporre che entrambe
le curve siano parametrizzate sull'intervallo $[0,1]$. Consideriamo
una qualunque funzione $p(t)\colon [0,1]\to [0,1]$ che sia:
strettamente crescente, di classe $\mathcal C^1$ e che abbia la
proprietà: $p'(0)=0$, $p'(1)=0$ (ad esempio $p(t)=3t^2-2t^3$). Allora
la curva $\alpha$ definita su $[0,2]$ in questo modo:
\[
\alpha(t) =
\begin{cases}
  \eta(p(t)) & \text{se $t\le 1$} \\
  \gamma(p(2-t)) & \text{se $t> 1$}
\end{cases}
\]
risulta essere una curva chiusa di classe $\mathcal C^1$ (osserviamo
però che $\alpha$ in generale non sarà una curva regolare perché la
derivata si annulla in $0$, $1$ e $2$). Essendo poi
\[
\int_\alpha \omega = \int_\eta \omega - \int_\gamma \omega
\]
si ottiene l'implicazione $2 \Rightarrow 3$.

L'implicazione $3 \Rightarrow 1$ è quella interessante. Si
tratta infatti di costruire una primitiva $f\colon \Omega\to \RR$.
Scelto un punto arbitrario $x_0\in \Omega$ definiamo:
\[
f(x) = \int_{\gamma_x} \omega
\]
dove $\gamma_x$ è una qualunque curva che congiune il punto $x_0$ con
il punto $x$. Per la proprietà 3 sappiamo che qualunque curva si
scelga il risultato ottenuto è lo stesso, quindi $f$ è ben definita.

Si tratta ora di verificare che $df = \omega$ ovvero che per ogni
$x\in \Omega$ e per ogni $v\in \RR^n$ si abbia:
\[
   \omega_x(v) = df_x(v).
\]
Ricordiamo che
\[
df_x(v) = \frac{\partial f}{\partial v}(x) = \lim_{h\to 0}
\frac{f(x+hv) - f(x)}{h}.
\]
Consideriamo quindi la quantità:
\[
 f(x+hv)-f(x) = \int_{\gamma_{x+hv}} \omega - \int_{\gamma_x} \omega.
\]
Scegliendo una curva $\gamma_x$ qualunque che congiunge $x_0$ a $x$ e
scegliendo come $\gamma_{x+hv}$ la curva che percorre $\gamma_x$ da
$x_0$ a $x$ e poi percorre il segmento\footnote{Possiamo certamente
  supporre che $h$ sia sufficientemente piccolo in modo tale che la
  palla centrata in $x$ di raggio $h|v|$ sia contenuta in $\Omega$ (in
quanto $\Omega$ è aperto) e quindi il segmento sia interamente
contenuto in $\Omega$.}
\[
  \gamma(t) = x + t v,\qquad t\in[0,h]
\]
da $x$ a $x+hv$, si ottiene:
\[
f(x+hv)-f(x) = \int_\gamma \omega = \int_0^h
\omega_{\gamma(t)}(\gamma'(t))\, dt
= \int_0^h \omega_{x+tv}(v)\, dt.
\]
Visto che $\omega_x(v)$ è una funzione continua nella variabile $x$,
possiamo applicare il teorema della media integrale per ottenere che
esiste un $\bar t \in [0,h]$ tale che
\[
\int_0^h \omega_{x+tv}(v)\, dt = h \omega_{x+\bar t v}(v).
\]
Osserviamo ora, di nuovo per la continuità di $\omega$, che per $h\to
0$ anche $\bar t \to 0$ e quindi $\omega_{x+\bar t v}(v) \to
\omega_x(v)$.
Mettendo assieme i risultati precedenti si ottiene quindi
\[
df_x(v) = \lim_{h\to 0}\frac{f(x+hv)-f(x)}{h} = \lim_{h\to 0}
\omega_{x+\bar tv}(v) = \omega_x(v)
\]
che è il risultato desiderato.
\end{proof}

Un sottoinsieme $E\subset \RR^2$ che può essere scritto nella forma
\[
  E = \{(x,y)\colon x\in [a,b],\ \phi(x) \le y \le \psi(x)\}
\]
dove $\phi,\psi\colon [a,b]\to \RR$ sono funzioni continue con $\phi(x)\le \psi(x)$ per ogni $x\in [a,b]$,
si dice essere un \emph{dominio normale rispetto all'asse delle
  $x$}. L'analoga definizione con le variabili $x$ e $y$ scambiate
ci dà i domini normali rispetto all'asse delle $y$.


\begin{theorem}[formule di Green]
Sia $E\subset \RR^2$ un dominio normale rispetto all'asse delle $x$
delimitato da due funzioni $\Phi(x)$, $\Psi(x)$ di classe $\mathcal C^1$. 
Allora, se $f$ è una funzione continua su $\overline E$ di classe $C^1$
in $E$, si ha:
\[
  -\int_E \frac{\partial f}{\partial y}(x,y)\, dx\, dy = \int_{\partial E} f(x,y)\, dx
\]
dove l'integrale sul lato destro dell'equazione si intende come la somma degli
integrali della forma differenziale $f(x,y)\, dx$ sulle quattro curve regolari che percorrono la frontiera di
$E$ in senso antiorario.

Se $E\subset \RR^2$ è un dominio normale rispetto all'asse delle $y$
si ha invece:
\[
  \int_E \frac{\partial f }{\partial x}(x,y)\, dx\, dy =
  \int_{\partial E} f(x,y)\, dy.
\]
\end{theorem}
\begin{proof}
Essendo $E$ un dominio normale rispetto all'asse delle $x$, le formule
di riduzione per gli integrali doppi, ci permettono di scrivere:
\[
\int_E \frac{\partial f}{\partial y}(x,y)\, dx\, dy
= \int_a^b \left(\int_{\psi(x)}^{\phi(x)} \frac{\partial f}{\partial
  y}\, dy \right)\, dx.
\]
Il teorema fondamentale del calcolo integrale ci permette di affermare
che
\[
 \int_{\psi(x)}^{\phi(x)} \frac{\partial f}{\partial
  y}(x,y) \, dy = f(x,\phi(x)) - f(x,\psi(x))
\]
da cui
\begin{equation}\label{eq:green_left}
- \int_E \frac{\partial f}{\partial y}(x,y)\, dx\, dy
=
-\int_a^b f(x,\phi(x))\, dx + \int_a^b f(x,\psi(x))\, dx.
\end{equation}

La frontiera $\partial
E$ è percorsa dalle seguenti quattro curve:
\begin{align*}
\gamma_1(t) &= (t,\psi(t)) \qquad t\in[a,b]\\
\gamma_2(t) &= (b,t) \qquad t\in[\psi(b),\phi(b)]\\
\gamma_3(t) &= (t,\phi(t)) \qquad t\in[a,b]\\
\gamma_4(t) &= (a,t) \qquad t\in[\psi(a),\phi(a)]
\end{align*}
dove le curve $\gamma_3$ e $\gamma_4$ sono percorse in senso inverso.
Si ha dunque
\begin{align*}
\int_{\partial E} f\, dx
= \int_{\gamma_1} f\, dx
 + \int_{\gamma_2} f\, dx
 - \int_{\gamma_3} f\, dx
 - \int_{\gamma_4} f\, dx.
\end{align*}
Osserviamo che sulle curve $\gamma_2$ e $\gamma_4$ l'integrale è
nullo, in quanto la curva ha coordinata $x$ costante. Sulle altre due
curve si ha invece $dx = dt$, cosicché:
\begin{equation}\label{eq:green_right}
\int_{\partial E} f(x,y)\, dx
= \int_a^b f(t,\psi(t))\, dt - \int_a^b f(t,\phi(t))\, dt.
\end{equation}
Mettendo assieme $\eqref{eq:green_left}$ e $\eqref{eq:green_right}$ si
ottiene la tesi del teorema.

La seconda parte del teorema si può dimostra in maniera analoga. Oppure, in
alternativa, si può osservare che scambiando le variabili $x$ e $y$,
l'integrale su $E$ rimane invariato, ma su $\partial E$ viene
invertita l'orientazione e quindi si ottiene un cambio di segno.
\end{proof}

\section{$k$-forme differenziali}

Una \emph{$k$-forma multilineare alternante} $\lambda\colon (\RR^n)^k \to \RR$ è
una funzione che associa ad una $k$-upla di vettori $v_1,\dots,v_k$
uno scalare $\lambda(v_1,\dots, v_k)$ che risulta essere lineare in
ogni singola componente $v_j$ e alternante, nel senso che scambiando
due vettori $v_i, v_j$ il risultato cambia segno.

\begin{example}
Per $n=3$, $k=2$ si può definire ad esempio:
\[
 \lambda(v, w) = v_1w_2 - v_2w_1
= \det\begin{pmatrix}
0 & v_1 & w_1\\
0 & v_2 & w_2\\
1 & v_3 & w_3
\end{pmatrix}
\]
dove $v=(v_1,v_2,v_3) \in \RR^3$, $w=(w_1,w_2,w_3) \in \RR^3$. Si può
verificare facilmente che $\lambda(v,w)$ è lineare sia rispetto a $v$
che rispetto a $w$. Inoltre $\lambda(w,v) = -\lambda(v,w)$ e dunque
$\lambda$ è un esempio di $2$-forma multilineare alternante (anche
detta forma bilineare alternante).
\end{example}

Per $k=1$ c'è un singolo vettore in ingresso e quindi si ottengono le
usuali applicazioni lineari: $\lambda \in (\RR^n)^*$. Per $k=0$ la definizione è
degenere ma si intende che $\lambda$ non è una funzione (in quanto
viene valutata su $0$ vettori) ma rappresenta un qualunque numero reale:
$\lambda\in \mathbb R$.

In generale,
le $k$-forme multilineari sono univocamente determinate dai valori che
assumono su tutte le $k$-uple di elementi della base canonica. Se
inoltre la forma è alternante, risulta univocamente determinata una
volta che si assegna il valore su una sola delle diverse permutazioni
di $k$ elementi della base canonica.

Per questo le $n$-forme alternanti in
$\RR^n$ sono univocamente determinate dal valore assunto sulla base
canonica $(\vec {e_1},\dots \vec{e_n})$ in quanto ogni scelta di $n$
vettori della base è una permutazione di questa.
Si osserva in effetti, che le proprietà
delle $n$-forme alternanti sono le stesse che identificano il determinante
quindi le
$n$-forme lineari alternanti su $\RR^n$ sono tutte multiple del
determinante, e formano uno spazio vettoriale di dimensione 1.

In generale lo spazio
vettoriale delle $k$-forme alternanti su $\RR^n$ ha dimensione ${n
  \choose k}$.

Se $\alpha$ è una $1$-forma e $\omega$ è una $k$-forma, possiamo
definire il loro prodotto esterno mimando lo sviluppo del
determinante:
\[
  (\alpha \wedge \omega)(v_1,\dots,v_{k+1}) 
  = \sum_{j=1}^{k+1}
(-1)^{j+1}\alpha(v_j) \omega(v_1,\dots, v_{j-1},v_{j+1},\dots,v_{k+1})
\]
L'operazione di prodotto esterno ci permette di costruire
$k$-forme moltiplicando tra loro, tramite prodotto esterno, le
$1$-forme. Il prodotto esterno risulta essere associativo, ma non
commutativo. Anzi, se $\alpha$ e $\beta$ sono $1$-forme, si ha
$\alpha\wedge \beta  = - \beta \wedge \alpha$.

Una funzione $\omega$ definita su un aperto $\Omega\subset \RR^n$ che
ad ogni $x\in \Omega$ associa una forma multilineare alternante, si
dice essere una $k$-forma differenziale su $\Omega$. L'insieme delle
$k$-forme differenziali si indica con $\Lambda^k(\Omega)$.

\begin{example}
Ricordiamo che nel piano $\RR^2$ una base delle $1$-forme è
data dalle applicazioni lineari $dx$ e $dy$.
Si ha allora:
\begin{align*}
  (dx\wedge dy)(\vec v,\vec w) & =dx(\vec v)dy(\vec w) - dx(\vec w)dy(\vec v) = v_1 w_2 - w_2 v_1\\
&=\det \begin{pmatrix}
v_1 & w_1 \\
v_2 & w_2
\end{pmatrix}
\end{align*}
e ogni $2$-forma in $\RR^2$ si scrive come
\[
\omega = f(x,y)\, dx\wedge dy.
\]
\end{example}


Le $2$-forme differenziali sono univocamente determinate una volta
assegnato il valore su ogni paio di vettori della base
canonica. Lo spazio vettoriale delle $2$-forme in $\RR^n$ risulta
quindi avere dimensione $n(n-1)/2$ in quanto una $2$-forma $\omega$ risulta
univocamente determinata dai valori $\omega(\vec {e_i}, \vec {e_j})$
con $1 \le i < j \le n$.

\begin{example}
Per $n=3$ lo spazio delle $2$-forme multilineari
alternanti ha dimensione
$n(n-1)/2=3=n$. Dunque le $2$-forme possono essere rappresentate
tramite vettori di $\RR^3$. In effetti se $\xi\in \RR^3$ osserviamo
che l'applicazione
\[
  \lambda(\vec v,\vec w) = (\xi, \vec v\wedge \vec w)
\]
risulta essere multilineare alternante. Dunque tutte le $2$-forme
differenziali $\omega \in \Lambda^2(\Omega)$ con $\Omega\subset \RR^3$
possono essere rappresentate da campi vettoriali $\xi\colon \Omega\to
\RR^3$ mediante l'identificazione:
\[
  \omega_x(\vec v, \vec w) = (\xi(x),\vec v \wedge \vec w).
\]
Andando ad esaminare le coordinate si trova che
\begin{align*}
  \omega(\vec{e_1},\vec{e_2}) = (\xi, \vec{e_1}\wedge\vec{e_2}) =
  (\xi,\vec{e_3}) = \xi_3\\
  \omega(\vec{e_2},\vec{e_3}) = (\xi, \vec{e_2}\wedge\vec{e_3}) =
  (\xi,\vec{e_1}) = \xi_1\\
  \omega(\vec{e_3},\vec{e_1}) = (\xi, \vec{e_3}\wedge\vec{e_1}) =
  (\xi,\vec{e_2}) = \xi_2
\end{align*}
da cui si ottiene
\[
 \omega = \xi_1\, dy\wedge dz + \xi_2\, dz\wedge dx + \xi_3\,
 dx\wedge dy.
\]
\end{example}


Se $\omega$ è una $k$-forma differenziale di classe $\mathcal C^1$ su
$\Omega$ vogliamo definire il \emph{differenziale} $d\omega$ che sarà
una $k+1$ forma differenziale. La definizione può essere data
imponendo che valga la relazione
\[
  d(f(x)\, \omega) = df(x) \wedge \omega
\]
ogni volta che $\omega$ è una forma differenziale costante (cioè che
non dipende da $x$).


Ad esempio se $\omega$ è una $1$-forma differenziale, si potrà
scrivere
\[
  \omega_x = \sum_{j=1}^n f_j(x) dx_j
\]
e il suo differenziale sarà:
\[
  (d\omega)_x = \sum_{j=1}^n (df_j)_x \wedge dx_j
  =\sum_{j=1}^n \sum_{i=1}^n \frac{\partial f_j}{\partial x_i}(x) dx_i
  \wedge dx_j.
\]
Si potranno poi cancellare i termini della somma con $i=j$ in quanto
$dx_i\wedge dx_j=0$ per la proprietà dell'alternanza. E si potranno
associare i termini con gli indici $i$ e $j$ scambiati, essendo $dx_i
\wedge dx_j = - dx_j \wedge dx_i$.

\section{$k$-forme nello spazio 3-dimensionale}

Se $\xi$ è un campo vettoriale definiamo la sua \emph{divergenza}
$\div \xi$ (anche indicata con $\vecnabla \cdot \xi$) tramite la formula
\[
  \div \xi = \tr( D\xi) = \sum_{k=1}^n \frac{\partial \xi_k}{\partial x_k}.
\]

Se $\omega_{(x,y,z)}$ è la $2$-forma in $\RR^3$ associata al campo vettoriale
$\xi(x,y,z)$ abbiamo visto che $\omega$ si può scrivere nel modo seguente:
\[
   \omega = \xi_1\, dy \wedge dz + \xi_2\, dz\wedge dx +
   \xi_3\, dx\wedge dy
\]
e quindi
\begin{align*}
  d \omega &=
  \frac{\partial \xi_1}{\partial x}\, dx\wedge dy\wedge dz +
  \frac{\partial \xi_2}{\partial y}\, dy\wedge dz\wedge dx +
  \frac{\partial \xi_3}{\partial z}\, dz\wedge dx\wedge dy \\
  & =
  \left(\frac{\partial \xi_1}{\partial x} +
  \frac{\partial \xi_2}{\partial y} +
  \frac{\partial \xi_3}{\partial z}\right)
  dx\wedge dy\wedge dz
  = (\div \xi)\, dx\wedge dy\wedge dz
\end{align*}
in quanto i temini che risultano avere dei differenziali ripetuti
(come $dy \wedge dy \wedge dz$) sono tutti nulli, mentre i prodotti
dei differenziali possono essere permutati ricordando che ogni scambio
comporta il cambio del segno. Abbiamo quindi osservato che il
differenziale delle $2$-forme corrisponde all'operatore divergenza nel
linguaggio dei campi vettoriali (più in generale questo è vero per le
$(n-1)$-forme differenziali in $\RR^n$).

Se $\xi$ è un campo vettoriale in $\RR^3$ definiamo
il suo \emph{rotore} $\rot \xi$ (anche indicato con $\vecnabla \wedge
\xi$)
tramite la formula:
\[
  (\rot \xi, \vec v) \defeq \div (\xi \wedge \vec v)
\]
da cui si ottiene
\begin{align*}
  \rot \xi = \sum_{j=1}^3 (\rot \xi,\vec{e_j})\vec{e_j}
 = \sum_{j=1}^3 \div(\xi \wedge \vec{e_j}) \vec{e_j}
 = \sum_{j=1}^3 \sum_{k=1}^3 \frac{\partial}{\partial x_k}(\vec {e_k},\xi\wedge
 \vec {e_j}) \vec{e_j}
\end{align*}
che formalmente si può scrivere nella forma:
\[
\text{``}\rot \xi = (\nabla,\xi \wedge \vec e)
= \det \begin{pmatrix}
\frac{\partial}{\partial x} & \xi_1 & \vec{e_1}\\
\frac{\partial}{\partial y} & \xi_2 & \vec{e_2}\\
\frac{\partial}{\partial z} & \xi_3 & \vec{e_3}
\end{pmatrix}
\text{''}.
\]
Se $\omega$ è la $1$-forma differenziale rappresentata dal
campo $\xi$, cioè
\[
 \omega = \xi_1 dx + \xi_2 dy + \xi_3 dz
\]
si ha
\begin{align*}
d \omega
&= \frac{\partial \xi_1}{\partial y}\, dy\wedge dx
+ \frac{\partial \xi_1}{\partial z}\, dz\wedge dx
+ \frac{\partial \xi_2}{\partial x}\, dx\wedge dy
\\
& \quad + \frac{\partial \xi_2}{\partial z}\, dz\wedge dy
+ \frac{\partial \xi_3}{\partial x}\, dx\wedge dz
+ \frac{\partial \xi_3}{\partial y}\, dy\wedge dz
\\
&=
\left(\frac{\partial \xi_2}{\partial x}-\frac{\partial \xi_1}{\partial y}\right) dx\wedge dy
+ \left(\frac{\partial \xi_3}{\partial x} - \frac{\partial \xi_1}{\partial z}\right) dx\wedge dz\\
&\quad + \left(\frac{\partial \xi_3}{\partial y} - \frac{\partial
  \xi_2}{\partial z}\right)  dy\wedge dz\\
&=  (\rot \xi)_3\, dx\wedge dy
  + (\rot \xi)_2\, dz\wedge dx
  + (\rot \xi)_1\, dy\wedge dz
\end{align*}
da cui si evince che se $\omega$ rappresenta il campo $\xi$, il
differenziale $d\omega$ rappresenta il campo $\rot \xi$.

Se apparentemente sia le $1$-forme di $\RR^3$ che le $2$-forme di
$\RR^3$ si possono rappresentare nello stesso modo, tramite un campo
vettoriale, le due categorie di oggetti sono in realtà molto diversi
tra loro. La differenza essendo come si trasformano questi oggetti
attraverso un cambio di variabili negli integrali.

\section{Integrale di una $k$-forma su una $k$-superficie}

Sia $\Phi\colon E\subset \RR^k\to \RR^n$ una funzione di classe
$\mathcal C^1$. L'insieme $S=\Phi(E)$ è un sottoinsieme di $\RR^n$ che
tramite la mappa $\Phi$ viene descritto al variare di $k$
parametri. Diremo che $S$ è una superficie parametrizzata da
$\Phi$. Se $\tilde\Phi \colon F\subset \RR^k\to \RR^n$ è un'altra
funzione tale per cui esiste una funzione bigettiva $\Psi\colon F\to
E$ di classe $\mathcal C^1$ con $\det D\Psi \neq 0$
che ci dà:
\[
  \tilde\Phi(\vec v) = \Phi(\Psi(\vec v)) \qquad \forall \vec v\in F
\]
allora diremo che $\tilde\Phi$ è una riparametrizzazione di $\Phi$ (o
anche, impropriamente, della superficie $S=\Phi(E) = \tilde\Phi(F)$) e
che $\Psi$ è il cambio di variabile corrispondente. Usualmente $\tilde
\Phi$ e $\Phi$ vengono identificati come la stessa \emph{superficie} e
si distinguono per il nome dato alle variabili:
\[
  x = \Phi(\vec u), \qquad \vec u = \Psi(\vec v), \qquad x =
  \tilde\Phi (\vec v) = \Phi(\Psi(\vec v)).
\]
Osserviamo che (se $F$ è connesso, come si supporrà sempre nel
seguito), il segno di $\det D\Psi$ è costante\footnote{altrimenti se ci
fossero due punti con $\det D\psi$ di segno opposto,
sulla curva che li congiunge la
funzione $\det D\Psi$, che è continua, dovrebbe annullarsi in almeno
un punto, per il teorema degli zeri.}. Diremo quindi che $\Phi$ e
$\tilde\Phi$ hanno la \emph{stessa orientazione} se $\det D\Psi>0$,
diremo invece che hanno \emph{orientazione opposta} se $\det D\Psi<0$.

Se $\omega$ è una $k$-forma differenziale definita su un aperto contenente
$S=\Phi(E)$, definiamo l'integrale di $\omega$ su $\Phi$ (o,
impropriamente, su $S$) come:
\[
\int_\Phi \omega \defeq \int_E \omega_{\Phi(\vec u)}\left(\frac{\partial
  \Phi}{\partial u_1}(\vec u), \dots, \frac{\partial \Phi}{\partial
  u_k}(\vec u)\right)\, du_1 \dots du_k.
\]

Se rappresentiamo i $k$ vettori argomento della $k$-forma, con una
matrice $n\times k$ le cui colonne sono i vettori, potremo scrivere
più semplicemente:
\[
  \omega\left(\frac{\partial
  \Phi}{\partial u_1}(\vec u), \dots, \frac{\partial \Phi}{\partial
    u_k}(\vec u)\right)
   = \omega(D\Phi).
\]
Ora se prendiamo il cambio di variabile $\Psi$, come sopra, osserviamo
che si ha, utilizzando la formula per il cambio di variabili $\vec u
= \Psi(\vec v)$, $du_1 \dots du_n = \lvert det D\Psi\rvert$
\begin{align*}
 \int_{\Phi} \omega & = \int_E \omega_{\Phi(\vec u)}(D\Phi(\vec u))\, du_1\dots du_k\\
 &= \int_F \omega_{\Phi(\Psi(\vec v))}(D\Phi(\Psi(\vec v)))\, \lvert
 \det D\Psi(\vec v)\rvert\, dv_1 \dots dv_k.
\end{align*}
A questo punto entrano finalmente in gioco le proprietà delle
$k$-forme alternanti. Come abbiamo già osservato nella dimostrazione
della formula dell'area per le applicazioni lineari, se $A$ è una
matrice $n\times k$ e $S$ è una
matrice $k\times k$ semplice \emph{destra} (ovvero una matrice che opera lo scambio di due
colonne, o la somma ad una colonna di un multiplo di un'altra colonna,
se moltiplicata sul lato destro)
allora dalle proprietà di alternanza, si deduce che
\[
  \omega(A S) = \omega(A)\det S.
\]
La proprietà di multilinearità di $\omega$ ci dice che
la stessa proprietà vale se $S$ è una matrice diagonale. Di
conseguenza, tramite l'algoritmo di riduzione di Gauss, possiamo
verificare che la proprietà vale per qualunque matrice $S$.

Dunque nel conto che stavamo facendo possiamo senz'altro dire che
$\omega(D\Phi)\lvert \det D\Psi\rvert = \pm \omega(D\Phi D\Psi)$ dove
il segno sarà positivo se il cambio di variabile mantiene
l'orientamento, negativo altrimenti.

Si avrà dunque:
\begin{align*}
\int_\Phi \omega
&= \pm \int_F \omega_{\tilde\Phi(\vec v)} (D\Phi(\Psi(\vec
v))D\Psi(\vec v))\, dv_1\dots dv_k \\
& = \pm \int_F \omega_{\tilde\Phi(\vec v)} (D\tilde\Phi(\vec v))\,
dv_1\dots dv_k
= \pm \int_{\tilde\Phi} \omega.
\end{align*}

Abbiamo quindi mostrato che l'integrale di una forma differenziale su
una superficie parametrizzata, non dipende dalla parametrizzazione e
si conserva se le parametrizzazioni hanno la stessa orientazione,
cambia invece segno se le parametrizzazioni hanno orientazione opposta.

Osserviamo anche che la notazione usata per le forme differenziali è
coerente con quella utilizzata negli integrali. Se infatti $\omega$ è
una $n$-forma in un insieme $E\subset \RR^n$ e prendiamo come
parametrizzazione $\Phi$ di $E$ l'identità, si ha
\[
\omega_x = f(x) dx_1\wedge \dots \wedge dx_n
\]
e
\[
\int_\Phi \omega
= \int_\Phi f(x)\, dx_1 \wedge \dots \wedge dx_n
= \int_E f(x)\, dx_1\dots dx_n.
\]

\begin{comment}
%%%%%%
%%%%%% SEZIONE NASCOSTA
%%%%%%
\section{Il teorema di Stokes}

Sia $\Phi\colon [0,1]^k \to \RR^n$. Il bordo del cubo $[0,1]^k$ è
formato da $2k$ facce, ognuna delle quali congruente a
$[0,1]^{k-1}$. Per ogni variabile $x_j$, $j=1,\dots,k$, i due piani
$x_j=1$ e $x_j=0$ determinano due facce opposte del cubo $[0,1]^k$.
Possiamo quindi parametrizzare il bordo del cubo tramite
le $2k$ mappe
$\Phi_j^\pm \colon [0,1]^{k-1}\to \RR^n$ nel modo seguente:
\begin{align*}
\Phi_j^+(u_1,\dots,u_{k-1}) &= \Phi(u_1,\dots,u_{j-1},1,u_j,\dots, u_{k-1})\\
\Phi_j^-(u_1,\dots,u_{k-1}) &= \Phi(u_1,\dots,u_{j-1},0,u_j,\dots, u_{k-1})
\end{align*}

\begin{theorem}[Stokes]
Sia  $\Phi\colon [0,1]^k\to \RR^n$ una funzione di classe $\mathcal
C^2$ e sia $\omega$ una $(k-1)$-forma differenziale definita su un
aperto contenente l'immagine di $\Phi$. Allora, definite $\Phi_j^\pm$
come sopra, si ha:
\begin{equation}\label{eq:stokes}
\int_\Phi d\omega = \sum_{j=1}^k \left((-1)^{j-1}\int_{\Phi_j^+} \omega +
(-1)^j \int_{\Phi_j^-} \omega\right).
\end{equation}

Formalmente, se identifichiamo la somma a membro destro come
l'integrale sulla frontiera $\partial \Phi$ della superficie, si potrà scrivere
\[
\int_\Phi d\omega = \int_{\partial \Phi} \omega.
\]
\end{theorem}
\begin{proof}
Procediamo per casi.

\emph{Caso 1.}
Supponiamo che sia $n=k$ e che $\Phi$ sia l'identità:
$\Phi(\vec u)=\vec u$ per ogni $u\in[0,1]^k$. Allora $\omega$, essendo una
$(k-1)$-forma in $\RR^k$, può sempre essere scritta in coordinate nel
modo seguente:
\[
\omega_{x} = \sum_{j=1}^k \omega_j
\]
con
\[
(\omega_j)_{x} = f_j(x)\, dx_1\wedge \dots \wedge dx_{j-1} \wedge dx_{j+1} \wedge \dots \wedge dx_k.
\]

Nel calcolare il differenziale di $\omega_j$, le derivate del termine
$f_j$ rispetto alla variabile $u_i$ danno luogo ad un differenziale in
cui $du_i$ compare due volte, se $i\neq j$. Dunque nel differenziale
di $\omega_j$ rimangono solo le derivate di $f_j$ rispetto a $u_j$:
\begin{align*}
(d\omega_j)_{x}
&= \frac{\partial f_j}{\partial x_j}(x)\,
dx_j \wedge dx_1\dots \wedge dx_{j-1} \wedge dx_{j+1} \wedge \dots
\wedge dx_k. \\
&= (-1)^{j-1}\frac{\partial f_j}{\partial x_j}(x)
dx_1\wedge \dots \wedge dx_k.
\end{align*}
Si ha dunque, scegliendo $x_j$ come prima variabile di integrazione e
applicando la formula fondamentale del calcolo integrale:
\begin{align*}
\int_{\Phi} d \omega_j
  &= (-1)^{j-1}\int_{[0,1]^k} \frac{\partial f_j}{\partial x_j}(x_1,\dots,x_n)
  \, dx_1 \dots dx_k\\
  &= (-1)^{j-1}\int_{[0,1]^{k-1}} \big(f_j(x_1,\dots,x_{j-1},1,x_{j+1},\dots,x_k)\\
  &\quad - f_j(x_1,\dots,x_{j-1},0,x_{j+1},\dots,x_k)\big)\,dx_1\dots dx_{j-1}\, dx_{j+1}\dots dx_k \\
  &=(-1)^{j-1} \int_{[0,1]^{k-1}}
  f_j(\Phi_j^+(\vec u))-f_j(\Phi_j^-(\vec u))\,
  du_1\dots du_k\\
  &= (-1)^{j-1} \int_{\Phi_j^+} \omega_j\  +\  (-1)^{j}
  \int_{\Phi_j^-} \omega_j\\
  &= (-1)^{j-1} \int_{\Phi_j^+} \omega \ + \ (-1)^{j} \int_{\Phi_j^-} \omega
\end{align*}
dove nell'ultima uguaglianza abbiamo sfruttato il fatto che per $i\neq
j$ si ha
\[
\int_{\Phi_j^\pm} \omega_i = \int_{\Phi_j^\pm} f_i\,dx_1\wedge \dots \wedge dx_{i-1}\wedge dx_{i+1}\wedge\dots
\wedge dx_k = 0
\]
in quanto $\Phi_j^\pm$ è costante rispetto alla variabile $x_j$ e quindi
\[
dx_1\wedge \dots \wedge dx_{i-1}\wedge dx_{i+1}\wedge\dots
\wedge dx_k(D\Phi_j^\pm(\vec u)) = 0.
\]
Sommando su $j=1,\dots,k$ si ottiene dunque la tesi.

\emph{Caso 2.}
Dimostriamo ora il caso generale con $\Phi\colon[0,1]^k\to \RR^n$ una
mappa di classe $\mathcal C^2$. Si tratta di osservare che data una
forma differenziale $\omega$ definita in $\RR^n$, esiste una forma
differenziale $\alpha$ definita su $[0,1]^k$ tale che
\[
  \int_{\mathrm{Id}} d\alpha = \int_\Phi d\omega
\qquad\text{e}\qquad
 \int_{\partial \mathrm{Id}} \alpha = \int_{\partial \Phi} \omega.
\]
Visto che nel caso precedente abbiamo dimostrato l'uguaglianza quando
$\Phi=\mathrm{Id}$, se valgono le relazioni sopra indicate si avrebbe
la conclusione nel caso generale.

In effetti possiamo definire
\[
 \alpha_{\vec u}(\vec{v_1},\dots,\vec{v_{k-1}}) = \omega_{\Phi(\vec
   u)}(\partial \Phi/\partial \vec{v_1}, \dots, \partial \Phi/\partial\vec{v_{k-1}})
\]
cosicché si ha, per definizione di integrale lungo $\Phi_j^\pm$,
\begin{align*}
  \int_{\Phi_j^\pm} \omega
  &= \int_{[0,1]^{k-1}} \omega_{\Phi_j^\pm(\vec v)} \left(\frac{\partial \Phi_j^\pm}{\partial v_1}, \dots , \frac{\partial \Phi_j^\pm}
  {\partial v_{k-1}}\right)\, dv_1\dots dv_{k-1}  \\
  &= \int_{F_k^\pm}
  \alpha_{\vec
    u}(\vec{e_1},\dots,\vec{e_{j-1}},\vec{e_{j+1}},\vec{e_{k-1}})\,
  du_1 \dots du_{j-1}\, du_{j+1} \dots du_{k-1}
\end{align*}
dove $F_k^+$ è la faccia del cubo $[0,1]^k$ dove $x_j=1$, mentre
$F_k^-$ e la faccia dove $x_j=0$.
\end{proof}
\end{comment} %%%%%%% END HIDE (SEZIONE NASCOSTA)

\section{Il teorema del rotore}
\begin{lemma}
\label{lm:triple_external_product}
Se $\vec u,\vec v, \vec w$ sono vettori di $\RR^3$, si ha:
\[
\vec u\wedge (\vec v \wedge \vec w)  = (\vec u,\vec w) \vec v - (\vec
u,\vec v) \vec w.
\]
\end{lemma}

\begin{lemma}
\label{lm:div_jacobian}
Se $\xi$ è un campo vettoriale e $\vec u$, $\vec v$ sono vettori di
$\RR^n$, si ha:
\[
(D\xi\, \vec u,\vec v) = \div ((\xi,\vec v)\vec u).
\]
\end{lemma}



\begin{theorem}[Kelvin-Stokes]
Sia $E\subset \RR^2$ un dominio normale rispetto ad entrambe le
variabili, la cui frontiera può essere percorsa in senso antiorario
da una curva $\gamma\colon[a,b]\to \partial E$ di classe $\C^1$ a tratti.
Sia $\Phi\colon E \to \RR^3$ una funzione di classe $\mathcal C^2$.
Dato $\xi\colon \Omega\subset \RR^3\to \RR^3$
un campo vettoriale di classe $\mathcal C^1$ definito su un aperto
$\Omega$ che contiene $[\Phi]=\Phi(E)$
si ha
\begin{align*}
\lefteqn{\int_E \left(\rot \xi(\Phi(u,v)),\frac{\partial \Phi}{\partial u}(u,v) \wedge \frac{\partial \Phi}{\partial v}(u,v)\right)\, du\, dv}\\
&=
\int_a^b (\xi(\Phi\circ \gamma(t)), (\Phi\circ \gamma)'(t))\, dt.
\end{align*}
\end{theorem}
Poniamo per brevità $\Phi_u = \frac{\partial \Phi}{\partial u}$ e
$\Phi_v = \frac{\partial \Phi}{\partial v}$. Abbiamo osservato altrove
che $\lvert \Phi_u\wedge \Phi_v\rvert = J(D\Phi)$. Inoltre se $\Phi_u
\wedge \Phi_v \neq \vec 0$ i vettori $\Phi_u$ e $\Phi_v$ sono
indipendenti e generano lo spazio vettoriale parallelo al piano
tangente alla superficie $S=[\Phi]=\Phi(E)$ nel punto $\Phi(u,v)$.
Il vettore $\Phi_u \wedge \Phi_v$ è quindi ortogonale al piano
tangente e potremo definire il versore normale come:
\[
 \nu_S = \frac{\Phi_u \wedge \Phi_v}{\lvert \Phi_u \wedge \Phi_v\rvert}.
\]
Si ha dunque:
\[
 \Phi_u \wedge \Phi_v = \nu_S\, J(D\Phi).
\]

Analogamente se $\gamma(t)$ è la curva che si ottiene concatendando
$\gamma_1$, $\gamma_2$, $\gamma_3$ e $\gamma_4$, e se $\gamma'(t) \neq \vec
0$, allora $\gamma'$ è un vettore tangente alla curva che descrive il
bordo della superficie $S$: $[\gamma]=\partial S$ nel
punto $\gamma(t)$. Possiamo quindi definire il versore tangente come
\[
  \tau_{\partial S} = \frac{\gamma'}{\lvert \gamma'\rvert}.
\]

Dunque se la parametrizzazione $\Phi$ è regolare (o comunque se i
punti non regolari hanno misura nulla e quindi non incidono sul valore
dell'integrale),
il teorema precedente si potrà quindi enunciare in modo indipendente
dalla parametrizzazione come segue:
\[
\int_S (\rot \xi, \nu_S)\, d\sigma = \int_{\partial S} (\xi,\tau)\, ds
\]
dove con $d\sigma$ si intende la misura di superficie (così come
introdotta negli integrali di superficie) e con $ds$ si intende la
lunghezza d'arco (così come introdotta negli integrali curvilinei).

\begin{proof}
Usiamo sempre la notazione $\Phi_u = \partial \Phi/\partial u$,
$\Phi_v = \partial \Phi/\partial v$ e definiamo
\begin{align*}
a(u,v) &\defeq \left(\xi(\Phi(u,v)),\Phi_u(u,v)\right)\\
b(u,v) &\defeq \left(\xi(\Phi(u,v)),\Phi_v(u,v)\right).
\end{align*}
Consideriamo quindi la forma differenziale $\omega = a\, du + b\, dv$
e calcoliamo il coefficiente di $d\omega$ (abbreviamo le notazioni
ricordando che le derivate di $\Phi$ sono sempre calcolate in $(u,v)$
mentre $\xi$ è calcolata in $\Phi(u,v)$):
\begin{align*}
\frac{\partial b}{\partial u} - \frac{\partial a}{\partial v}
& = \frac{\partial}{\partial u}(\xi,\Phi_v) -\frac{\partial}{\partial
  v}(\xi,\Phi_u)\\
& = \left(\frac{\partial}{\partial u} \xi\circ \Phi, \Phi_v\right) +
  (\xi,\Phi_{uv}) - \left(\frac{\partial}{\partial v} \xi \circ \Phi,
  \Phi_u\right)
  -(\xi,\Phi_{vu})\\
& = (D\xi \Phi_u, \Phi_v)-(D\xi \Phi_v,\Phi_u)\\
& = \div (\xi,\Phi_v) \Phi_u - \div (\xi,\Phi_u) \Phi_v\\
& = \div ( (\xi,\Phi_v) \Phi u - (\xi,\Phi_u) \Phi_v)\\
& = \div (\xi \wedge (\Phi_u \wedge \Phi_v))\\
& = (\rot \xi, \Phi_u\wedge \Phi_v).
\end{align*}
Nella derivazione precedente abbiamo usato il
Lemma~\ref{lm:triple_external_product} e il Lemma~\ref{lm:div_jacobian}

Posto $\gamma(t) = (u(t),v(t))$,
le formule di Green ci dicono che vale
\begin{align*}
\lefteqn{\int_E (\rot \xi, \Phi_u \wedge \Phi_v)}\\
 & = \int_E  \frac{\partial b}{\partial u} - \frac{\partial
   a}{\partial v}\, du\,dv
  = \int_{\gamma} a(u,v)\, du + b(u,v)\, dv\\
 & = \int_a^b a(\gamma(t)) u'(t) + b(\gamma(t))v'(t) \, dt\\
 & = \int_a^b (\xi(\Phi(\gamma(t))), \Phi_u(\gamma(t)) u'(t)
     + (\xi(\Phi(\gamma(t))), \Phi_v(\gamma(t)) v'(t)\,
 dt \\
 & = \int_a^b (\xi(\Phi(\gamma(t))), (\Phi\circ\gamma)'(t))\, dt.
\end{align*}
che è la formula che si voleva dimostrare.
\end{proof}

\section*{Modifiche}
\begin{itemize}
\item[26.10.2014] Prima stesura.
\item[10.2.2015] Piccole aggiunte (chiarificazioni).
\item[2.12.2023] Correzioni segnalate da Luca Milanese.
\end{itemize}


\end{document}
